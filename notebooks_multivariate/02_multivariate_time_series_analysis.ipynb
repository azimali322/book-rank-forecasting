{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Multivariate Time Series Analysis\n",
    "\n",
    "This notebook focuses on:\n",
    "1. Time series decomposition for multiple variables\n",
    "2. Stationarity testing for multivariate series\n",
    "3. Cross-correlation analysis between features\n",
    "4. Granger causality testing\n",
    "5. Cointegration analysis\n",
    "6. Feature engineering for multivariate time series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries with robust error handling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import os\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All imports completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared data from previous notebook\n",
    "print(\"=== LOADING PREPARED DATA ===\")\n",
    "\n",
    "try:\n",
    "    # Load the prepared data splits\n",
    "    in_time_data = pd.read_csv('../data_multivariate/in_time_features.csv', index_col=0, parse_dates=True)\n",
    "    out_of_time_data = pd.read_csv('../data_multivariate/out_of_time_features.csv', index_col=0, parse_dates=True)\n",
    "    \n",
    "    print(f\"✅ Data loaded successfully!\")\n",
    "    print(f\"In-time data shape: {in_time_data.shape}\")\n",
    "    print(f\"Out-of-time data shape: {out_of_time_data.shape}\")\n",
    "    print(f\"Date range: {in_time_data.index.min()} to {out_of_time_data.index.max()}\")\n",
    "    \n",
    "    # Set up dataset configuration\n",
    "    target = 'PM2.5'  # Default target for air quality data\n",
    "    feature_columns = [col for col in in_time_data.columns if col != target]\n",
    "    \n",
    "    print(f\"Features: {len(feature_columns)}\")\n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Feature columns: {feature_columns}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Prepared data files not found\")\n",
    "    print(\"Please run 01_multivariate_data_exploration.ipynb first\")\n",
    "    in_time_data = None\n",
    "    out_of_time_data = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate Time Series Decomposition\n",
    "print(\"=== MULTIVARIATE TIME SERIES DECOMPOSITION ===\")\n",
    "\n",
    "if in_time_data is not None and not in_time_data.empty:\n",
    "    # Select key variables for decomposition analysis\n",
    "    key_variables = [target] + feature_columns[:3]  # Target + first 3 features\n",
    "    \n",
    "    print(f\"Analyzing decomposition for: {key_variables}\")\n",
    "    \n",
    "    # Test different periods for seasonal patterns\n",
    "    periods_to_test = [24, 168, 720]  # Daily (24h), Weekly (168h), Monthly (720h)\n",
    "    \n",
    "    decomposition_results = {}\n",
    "    \n",
    "    for variable in key_variables:\n",
    "        if variable in in_time_data.columns:\n",
    "            print(f\"\\n--- {variable} Decomposition ---\")\n",
    "            \n",
    "            ts_data = in_time_data[variable].dropna()\n",
    "            if len(ts_data) < 100:  # Need sufficient data\n",
    "                print(f\"Skipping {variable}: insufficient data ({len(ts_data)} points)\")\n",
    "                continue\n",
    "            \n",
    "            variable_results = {}\n",
    "            \n",
    "            for period in periods_to_test:\n",
    "                if period >= len(ts_data) // 2:\n",
    "                    print(f\"Period {period}: Skipped (too large for data length {len(ts_data)})\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Test both additive and multiplicative models\n",
    "                    for model_type in ['additive', 'multiplicative']:\n",
    "                        try:\n",
    "                            decomposition = seasonal_decompose(ts_data, model=model_type, period=period)\n",
    "                            \n",
    "                            # Calculate component strengths\n",
    "                            trend_strength = np.var(decomposition.trend.dropna()) / np.var(decomposition.observed)\n",
    "                            seasonal_strength = np.var(decomposition.seasonal.dropna()) / np.var(decomposition.observed)\n",
    "                            residual_strength = np.var(decomposition.resid.dropna()) / np.var(decomposition.observed)\n",
    "                            \n",
    "                            variable_results[f'period_{period}_{model_type}'] = {\n",
    "                                'trend_strength': trend_strength,\n",
    "                                'seasonal_strength': seasonal_strength,\n",
    "                                'residual_strength': residual_strength,\n",
    "                                'decomposition': decomposition\n",
    "                            }\n",
    "                            \n",
    "                            print(f\"  Period {period} ({model_type}): Trend={trend_strength:.4f}, Seasonal={seasonal_strength:.4f}, Residual={residual_strength:.4f}\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"  Period {period} ({model_type}): Failed - {str(e)[:50]}...\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"  Period {period}: Failed - {str(e)[:50]}...\")\n",
    "            \n",
    "            decomposition_results[variable] = variable_results\n",
    "    \n",
    "    print(f\"\\n✅ Decomposition analysis completed for {len(decomposition_results)} variables\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No data available for decomposition analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stationarity Testing for Multivariate Series\n",
    "print(\"=== STATIONARITY TESTING FOR MULTIVARIATE SERIES ===\")\n",
    "\n",
    "def test_stationarity_multivariate(series, name='Time Series'):\n",
    "    \"\"\"\n",
    "    Test stationarity using ADF and KPSS tests for multivariate series\n",
    "    \"\"\"\n",
    "    print(f'\\nResults for {name}:')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # ADF Test\n",
    "    adf_result = adfuller(series.dropna())\n",
    "    print('ADF Statistic:', adf_result[0])\n",
    "    print('p-value:', adf_result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f'\\t{key}: {value}')\n",
    "    \n",
    "    adf_stationary = adf_result[1] <= 0.05\n",
    "    if adf_stationary:\n",
    "        print(\"ADF Test: Series is stationary\")\n",
    "    else:\n",
    "        print(\"ADF Test: Series is non-stationary\")\n",
    "    \n",
    "    print('\\n' + '-' * 30)\n",
    "    \n",
    "    # KPSS Test\n",
    "    kpss_result = kpss(series.dropna())\n",
    "    print('KPSS Statistic:', kpss_result[0])\n",
    "    print('p-value:', kpss_result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in kpss_result[3].items():\n",
    "        print(f'\\t{key}: {value}')\n",
    "    \n",
    "    kpss_stationary = kpss_result[1] >= 0.05\n",
    "    if kpss_stationary:\n",
    "        print(\"KPSS Test: Series is stationary\")\n",
    "    else:\n",
    "        print(\"KPSS Test: Series is non-stationary\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('INTERPRETATION:')\n",
    "    if adf_stationary and kpss_stationary:\n",
    "        print('✅ Both tests agree: Series is STATIONARY')\n",
    "    elif not adf_stationary and not kpss_stationary:\n",
    "        print('✅ Both tests agree: Series is NON-STATIONARY')\n",
    "    else:\n",
    "        print('⚠️  Tests disagree: Series is likely TREND-STATIONARY')\n",
    "    \n",
    "    return {\n",
    "        'adf_statistic': adf_result[0],\n",
    "        'adf_pvalue': adf_result[1],\n",
    "        'adf_stationary': adf_stationary,\n",
    "        'kpss_statistic': kpss_result[0],\n",
    "        'kpss_pvalue': kpss_result[1],\n",
    "        'kpss_stationary': kpss_stationary\n",
    "    }\n",
    "\n",
    "if in_time_data is not None and not in_time_data.empty:\n",
    "    # Test stationarity for all variables\n",
    "    stationarity_results = {}\n",
    "    \n",
    "    print(\"Testing stationarity for all variables...\")\n",
    "    \n",
    "    for column in in_time_data.columns:\n",
    "        if in_time_data[column].dtype in ['float64', 'int64']:\n",
    "            print(f\"\\n\" + \"=\"*60)\n",
    "            print(f\"TESTING STATIONARITY: {column.upper()}\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            stationarity_results[column] = test_stationarity_multivariate(\n",
    "                in_time_data[column], \n",
    "                f'{column} (Original)'\n",
    "            )\n",
    "            \n",
    "            # Test first difference if original is non-stationary\n",
    "            if not (stationarity_results[column]['adf_stationary'] and \n",
    "                   stationarity_results[column]['kpss_stationary']):\n",
    "                \n",
    "                print(f\"\\n\" + \"=\"*60)\n",
    "                print(f\"TESTING FIRST DIFFERENCE: {column.upper()}\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                diff_series = in_time_data[column].diff().dropna()\n",
    "                if len(diff_series) > 0:\n",
    "                    stationarity_results[f'{column}_diff'] = test_stationarity_multivariate(\n",
    "                        diff_series, \n",
    "                        f'{column} (First Difference)'\n",
    "                    )\n",
    "    \n",
    "    # Summary of stationarity results\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"STATIONARITY TESTING SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary_data = []\n",
    "    for var_name, results in stationarity_results.items():\n",
    "        summary_data.append({\n",
    "            'Variable': var_name,\n",
    "            'ADF_Stationary': results['adf_stationary'],\n",
    "            'KPSS_Stationary': results['kpss_stationary'],\n",
    "            'ADF_pvalue': results['adf_pvalue'],\n",
    "            'KPSS_pvalue': results['kpss_pvalue'],\n",
    "            'Overall_Stationary': results['adf_stationary'] and results['kpss_stationary']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Count stationary vs non-stationary\n",
    "    stationary_count = summary_df['Overall_Stationary'].sum()\n",
    "    total_count = len(summary_df)\n",
    "    \n",
    "    print(f\"\\nStationarity Summary:\")\n",
    "    print(f\"Stationary series: {stationary_count}/{total_count}\")\n",
    "    print(f\"Non-stationary series: {total_count - stationary_count}/{total_count}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No data available for stationarity testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Correlation Analysis and Granger Causality Testing\n",
    "print(\"=== CROSS-CORRELATION ANALYSIS AND GRANGER CAUSALITY TESTING ===\")\n",
    "\n",
    "if in_time_data is not None and not in_time_data.empty:\n",
    "    # Cross-correlation analysis\n",
    "    print(\"1. CROSS-CORRELATION ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Calculate cross-correlations between target and features\n",
    "    if target in in_time_data.columns:\n",
    "        cross_correlations = {}\n",
    "        \n",
    "        for feature in feature_columns:\n",
    "            if feature in in_time_data.columns:\n",
    "                # Calculate cross-correlation at different lags\n",
    "                target_series = in_time_data[target].dropna()\n",
    "                feature_series = in_time_data[feature].dropna()\n",
    "                \n",
    "                # Align series\n",
    "                common_index = target_series.index.intersection(feature_series.index)\n",
    "                target_aligned = target_series.loc[common_index]\n",
    "                feature_aligned = feature_series.loc[common_index]\n",
    "                \n",
    "                # Calculate cross-correlation\n",
    "                correlation = np.corrcoef(target_aligned, feature_aligned)[0, 1]\n",
    "                cross_correlations[feature] = correlation\n",
    "                \n",
    "                print(f\"  {feature} ↔ {target}: {correlation:.4f}\")\n",
    "        \n",
    "        # Sort by absolute correlation\n",
    "        sorted_correlations = sorted(cross_correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        print(f\"\\nTop correlations with {target}:\")\n",
    "        for feature, corr in sorted_correlations[:5]:\n",
    "            direction = \"positive\" if corr > 0 else \"negative\"\n",
    "            print(f\"  {feature}: {corr:.4f} ({direction})\")\n",
    "    \n",
    "    # Granger Causality Testing\n",
    "    print(f\"\\n2. GRANGER CAUSALITY TESTING\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if target in in_time_data.columns and len(feature_columns) > 0:\n",
    "        print(\"Testing Granger causality from features to target...\")\n",
    "        \n",
    "        granger_results = {}\n",
    "        max_lags = min(10, len(in_time_data) // 20)  # Conservative lag selection\n",
    "        \n",
    "        for feature in feature_columns[:5]:  # Test top 5 features to avoid overloading\n",
    "            if feature in in_time_data.columns:\n",
    "                try:\n",
    "                    # Prepare data for Granger causality test\n",
    "                    target_series = in_time_data[target].dropna()\n",
    "                    feature_series = in_time_data[feature].dropna()\n",
    "                    \n",
    "                    # Align series\n",
    "                    common_index = target_series.index.intersection(feature_series.index)\n",
    "                    target_aligned = target_series.loc[common_index]\n",
    "                    feature_aligned = feature_series.loc[common_index]\n",
    "                    \n",
    "                    # Create DataFrame for Granger test\n",
    "                    test_data = pd.DataFrame({\n",
    "                        'target': target_aligned,\n",
    "                        'feature': feature_aligned\n",
    "                    }).dropna()\n",
    "                    \n",
    "                    if len(test_data) > max_lags * 2:  # Need sufficient data\n",
    "                        # Perform Granger causality test\n",
    "                        gc_result = grangercausalitytests(test_data[['target', 'feature']], \n",
    "                                                        maxlag=max_lags, \n",
    "                                                        verbose=False)\n",
    "                        \n",
    "                        # Extract p-values for different lags\n",
    "                        p_values = []\n",
    "                        for lag in range(1, min(max_lags + 1, len(gc_result) + 1)):\n",
    "                            if lag in gc_result:\n",
    "                                p_value = gc_result[lag][0]['ssr_ftest'][1]  # F-test p-value\n",
    "                                p_values.append(p_value)\n",
    "                        \n",
    "                        if p_values:\n",
    "                            min_p_value = min(p_values)\n",
    "                            granger_results[feature] = {\n",
    "                                'min_p_value': min_p_value,\n",
    "                                'significant': min_p_value < 0.05,\n",
    "                                'p_values': p_values\n",
    "                            }\n",
    "                            \n",
    "                            significance = \"Significant\" if min_p_value < 0.05 else \"Not significant\"\n",
    "                            print(f\"  {feature} → {target}: p-value = {min_p_value:.4f} ({significance})\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  {feature} → {target}: Test failed - {str(e)[:50]}...\")\n",
    "        \n",
    "        # Summary of Granger causality\n",
    "        if granger_results:\n",
    "            significant_features = [f for f, r in granger_results.items() if r['significant']]\n",
    "            print(f\"\\nGranger Causality Summary:\")\n",
    "            print(f\"  Features with significant causality to {target}: {len(significant_features)}\")\n",
    "            if significant_features:\n",
    "                print(f\"  Significant features: {significant_features}\")\n",
    "            else:\n",
    "                print(f\"  No significant Granger causality found\")\n",
    "    \n",
    "    print(f\"\\n✅ Cross-correlation and Granger causality analysis completed\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No data available for cross-correlation analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Next Steps\n",
    "print(\"=== MULTIVARIATE TIME SERIES ANALYSIS COMPLETE ===\")\n",
    "\n",
    "if in_time_data is not None and not in_time_data.empty:\n",
    "    print(\"✅ Multivariate time series analysis completed\")\n",
    "    print(f\"✅ Dataset analyzed: {len(in_time_data)} observations\")\n",
    "    print(f\"✅ Variables analyzed: {len(in_time_data.columns)}\")\n",
    "    print(f\"✅ Time series decomposition performed\")\n",
    "    print(f\"✅ Stationarity testing completed\")\n",
    "    print(f\"✅ Cross-correlation analysis performed\")\n",
    "    print(f\"✅ Granger causality testing completed\")\n",
    "    \n",
    "    print(f\"\\n=== KEY FINDINGS ===\")\n",
    "    \n",
    "    # Stationarity summary\n",
    "    if 'stationarity_results' in locals():\n",
    "        stationary_count = sum(1 for r in stationarity_results.values() \n",
    "                              if r['adf_stationary'] and r['kpss_stationary'])\n",
    "        total_count = len(stationarity_results)\n",
    "        print(f\"• Stationary series: {stationary_count}/{total_count}\")\n",
    "    \n",
    "    # Cross-correlation summary\n",
    "    if 'cross_correlations' in locals():\n",
    "        strong_correlations = sum(1 for corr in cross_correlations.values() if abs(corr) > 0.5)\n",
    "        print(f\"• Strong correlations (|r| > 0.5): {strong_correlations}/{len(cross_correlations)}\")\n",
    "    \n",
    "    # Granger causality summary\n",
    "    if 'granger_results' in locals():\n",
    "        significant_causality = sum(1 for r in granger_results.values() if r['significant'])\n",
    "        print(f\"• Significant Granger causality: {significant_causality}/{len(granger_results)}\")\n",
    "    \n",
    "    print(f\"\\n=== NEXT STEPS ===\")\n",
    "    print(\"1. Proceed to 03_multivariate_model_training.ipynb\")\n",
    "    print(\"   - VAR (Vector Autoregression) models\")\n",
    "    print(\"   - VARMA (Vector ARMA) models\") \n",
    "    print(\"   - Machine learning models with multiple features\")\n",
    "    print(\"   - Deep learning approaches (LSTM, GRU)\")\n",
    "    print(\"   - Feature engineering for multivariate forecasting\")\n",
    "    \n",
    "    print(\"\\n2. Continue to 04_multivariate_model_evaluation.ipynb\")\n",
    "    print(\"   - Model comparison and selection\")\n",
    "    print(\"   - Ensemble methods for multivariate forecasting\")\n",
    "    print(\"   - Cross-validation for time series\")\n",
    "    print(\"   - Final model deployment recommendations\")\n",
    "    \n",
    "    print(f\"\\n🎯 Ready to proceed with multivariate model training!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Analysis incomplete\")\n",
    "    print(\"Please ensure the data is loaded correctly from the previous notebook\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
