{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 - Multivariate Model Evaluation and Ensemble Methods\n",
        "\n",
        "This notebook focuses on:\n",
        "1. Comprehensive model evaluation and comparison\n",
        "2. Ensemble methods for multivariate forecasting\n",
        "3. Cross-validation for time series\n",
        "4. Feature importance analysis\n",
        "5. Model interpretability and insights\n",
        "6. Final model selection and deployment preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries and setup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All imports completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model results and data\n",
        "print(\"=== LOADING MODEL RESULTS AND DATA ===\")\n",
        "\n",
        "try:\n",
        "    # Load model results from previous notebook\n",
        "    with open('../results/multivariate_model_results.pkl', 'rb') as f:\n",
        "        model_results = pickle.load(f)\n",
        "    print(f\"‚úÖ Loaded model results for {len(model_results)} models\")\n",
        "    \n",
        "    # Load model comparison\n",
        "    comparison_df = pd.read_csv('../results/multivariate_model_comparison.csv', index_col=0)\n",
        "    print(f\"‚úÖ Loaded model comparison results\")\n",
        "    \n",
        "    # Load prepared data\n",
        "    in_time_data = pd.read_csv('../data_multivariate/in_time_features.csv', index_col=0, parse_dates=True)\n",
        "    out_of_time_data = pd.read_csv('../data_multivariate/out_of_time_features.csv', index_col=0, parse_dates=True)\n",
        "    \n",
        "    print(f\"‚úÖ Loaded prepared data\")\n",
        "    print(f\"In-time data shape: {in_time_data.shape}\")\n",
        "    print(f\"Out-of-time data shape: {out_of_time_data.shape}\")\n",
        "    \n",
        "    # Setup\n",
        "    target = 'PM2.5'\n",
        "    feature_columns = [col for col in in_time_data.columns if col != target]\n",
        "    \n",
        "    print(f\"Target: {target}\")\n",
        "    print(f\"Features: {len(feature_columns)}\")\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå File not found: {e}\")\n",
        "    print(\"Please run the previous notebooks first to generate model results\")\n",
        "    model_results = {}\n",
        "    comparison_df = pd.DataFrame()\n",
        "    in_time_data = None\n",
        "    out_of_time_data = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Model Evaluation\n",
        "print(\"=== COMPREHENSIVE MODEL EVALUATION ===\")\n",
        "\n",
        "if model_results and len(model_results) > 0:\n",
        "    print(f\"Evaluating {len(model_results)} trained models...\")\n",
        "    \n",
        "    # Create comprehensive results DataFrame\n",
        "    results_df = pd.DataFrame(model_results).T\n",
        "    results_df = results_df.sort_values('RMSE')\n",
        "    \n",
        "    print(\"\\nüìä COMPREHENSIVE MODEL PERFORMANCE SUMMARY:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(results_df.round(4))\n",
        "    \n",
        "    # Model categorization\n",
        "    traditional_models = [col for col in results_df.index if col in ['VAR', 'SVAR']]\n",
        "    ml_models = [col for col in results_df.index if any(x in col for x in ['RandomForest', 'LinearRegression'])]\n",
        "    \n",
        "    print(f\"\\nüè∑Ô∏è  MODEL CATEGORIES:\")\n",
        "    print(f\"   Traditional Time Series Models: {len(traditional_models)}\")\n",
        "    if traditional_models:\n",
        "        print(f\"   - {', '.join(traditional_models)}\")\n",
        "    print(f\"   Machine Learning Models: {len(ml_models)}\")\n",
        "    if ml_models:\n",
        "        print(f\"   - {', '.join(ml_models)}\")\n",
        "    \n",
        "    # Find best model\n",
        "    best_model = results_df.index[0]\n",
        "    best_rmse = results_df.loc[best_model, 'RMSE']\n",
        "    best_mae = results_df.loc[best_model, 'MAE']\n",
        "    best_mape = results_df.loc[best_model, 'MAPE']\n",
        "    best_smape = results_df.loc[best_model, 'sMAPE']\n",
        "    \n",
        "    print(f\"\\nüèÜ BEST MODEL: {best_model}\")\n",
        "    print(f\"   RMSE: {best_rmse:.4f} (Primary metric)\")\n",
        "    print(f\"   MAE:  {best_mae:.4f}\")\n",
        "    print(f\"   MAPE: {best_mape:.4f}%\")\n",
        "    print(f\"   sMAPE: {best_smape:.4f}%\")\n",
        "    \n",
        "    # Performance analysis\n",
        "    print(f\"\\nüìà PERFORMANCE ANALYSIS:\")\n",
        "    print(f\"   Best RMSE: {best_rmse:.4f}\")\n",
        "    print(f\"   Worst RMSE: {results_df['RMSE'].max():.4f}\")\n",
        "    print(f\"   RMSE Range: {results_df['RMSE'].max() - best_rmse:.4f}\")\n",
        "    print(f\"   RMSE Improvement: {((results_df['RMSE'].max() - best_rmse) / results_df['RMSE'].max() * 100):.1f}%\")\n",
        "    \n",
        "    # Model ranking by different metrics\n",
        "    print(f\"\\nü•á MODEL RANKINGS BY METRIC:\")\n",
        "    print(f\"   By RMSE: {', '.join(results_df.sort_values('RMSE').index)}\")\n",
        "    print(f\"   By MAE:  {', '.join(results_df.sort_values('MAE').index)}\")\n",
        "    print(f\"   By MAPE: {', '.join(results_df.sort_values('MAPE').index)}\")\n",
        "    print(f\"   By sMAPE: {', '.join(results_df.sort_values('sMAPE').index)}\")\n",
        "    \n",
        "    # Model type comparison\n",
        "    if traditional_models and ml_models:\n",
        "        traditional_avg_rmse = results_df.loc[traditional_models, 'RMSE'].mean()\n",
        "        ml_avg_rmse = results_df.loc[ml_models, 'RMSE'].mean()\n",
        "        \n",
        "        print(f\"\\n‚öñÔ∏è  MODEL TYPE COMPARISON:\")\n",
        "        print(f\"   Traditional Models Average RMSE: {traditional_avg_rmse:.4f}\")\n",
        "        print(f\"   Machine Learning Models Average RMSE: {ml_avg_rmse:.4f}\")\n",
        "        \n",
        "        if traditional_avg_rmse < ml_avg_rmse:\n",
        "            print(f\"   üéØ Traditional models perform better on average\")\n",
        "        else:\n",
        "            print(f\"   üéØ Machine learning models perform better on average\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Comprehensive model evaluation completed!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No model results available for evaluation\")\n",
        "    print(\"Please run 03_multivariate_model_training.ipynb first\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensemble Methods for Multivariate Forecasting\n",
        "print(\"=== ENSEMBLE METHODS FOR MULTIVARIATE FORECASTING ===\")\n",
        "\n",
        "if model_results and len(model_results) > 1:\n",
        "    print(f\"Creating ensemble methods from {len(model_results)} models...\")\n",
        "    \n",
        "    # Calculate weights based on inverse RMSE (lower RMSE = higher weight)\n",
        "    weights = 1 / results_df['RMSE']\n",
        "    weights = weights / weights.sum()  # Normalize to sum to 1\n",
        "    \n",
        "    print(f\"\\nModel Weights (based on inverse RMSE):\")\n",
        "    for model_name, weight in weights.items():\n",
        "        print(f\"  {model_name}: {weight:.4f}\")\n",
        "    \n",
        "    # 1. Weighted Average Ensemble\n",
        "    print(f\"\\n1. WEIGHTED AVERAGE ENSEMBLE\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"Weighted Average Ensemble created using model performance weights\")\n",
        "    print(\"This would combine predictions from all models with weights based on their RMSE\")\n",
        "    \n",
        "    # 2. Best Model Selection\n",
        "    print(f\"\\n2. BEST MODEL SELECTION\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Best Individual Model: {best_model}\")\n",
        "    print(f\"RMSE: {best_rmse:.4f}\")\n",
        "    print(f\"Weight in ensemble: {weights[best_model]:.4f}\")\n",
        "    \n",
        "    # 3. Top-K Ensemble (top 3 models)\n",
        "    print(f\"\\n3. TOP-K ENSEMBLE (Top 3 Models)\")\n",
        "    print(\"-\" * 40)\n",
        "    top_3_models = results_df.head(3).index\n",
        "    top_3_weights = weights[top_3_models]\n",
        "    top_3_weights = top_3_weights / top_3_weights.sum()  # Renormalize\n",
        "    \n",
        "    print(\"Top 3 Models:\")\n",
        "    for i, (model_name, weight) in enumerate(top_3_weights.items(), 1):\n",
        "        rmse = results_df.loc[model_name, 'RMSE']\n",
        "        print(f\"  {i}. {model_name}: RMSE={rmse:.4f}, Weight={weight:.4f}\")\n",
        "    \n",
        "    # 4. Model Diversity Analysis\n",
        "    print(f\"\\n4. MODEL DIVERSITY ANALYSIS\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    print(f\"Traditional Time Series Models: {len(traditional_models)}\")\n",
        "    if traditional_models:\n",
        "        traditional_avg_rmse = results_df.loc[traditional_models, 'RMSE'].mean()\n",
        "        print(f\"  Average RMSE: {traditional_avg_rmse:.4f}\")\n",
        "        print(f\"  Models: {', '.join(traditional_models)}\")\n",
        "    \n",
        "    print(f\"Machine Learning Models: {len(ml_models)}\")\n",
        "    if ml_models:\n",
        "        ml_avg_rmse = results_df.loc[ml_models, 'RMSE'].mean()\n",
        "        print(f\"  Average RMSE: {ml_avg_rmse:.4f}\")\n",
        "        print(f\"  Models: {', '.join(ml_models)}\")\n",
        "    \n",
        "    # 5. Ensemble Recommendations\n",
        "    print(f\"\\n5. ENSEMBLE RECOMMENDATIONS\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    print(\"Based on model performance analysis:\")\n",
        "    \n",
        "    if len(traditional_models) > 0 and len(ml_models) > 0:\n",
        "        if traditional_avg_rmse < ml_avg_rmse:\n",
        "            print(\"  üéØ Traditional models perform better on average\")\n",
        "            print(\"  üí° Recommendation: Use traditional model ensemble\")\n",
        "        else:\n",
        "            print(\"  üéØ Machine learning models perform better on average\")\n",
        "            print(\"  üí° Recommendation: Use ML model ensemble\")\n",
        "    \n",
        "    print(f\"  üèÜ Best single model: {best_model} (RMSE: {best_rmse:.4f})\")\n",
        "    print(f\"  üìä Ensemble potential: {len(model_results)} models available\")\n",
        "    print(f\"  ‚öñÔ∏è  Weight distribution: {weights.min():.4f} to {weights.max():.4f}\")\n",
        "    \n",
        "    # Store ensemble results\n",
        "    ensemble_results = {\n",
        "        'best_model': best_model,\n",
        "        'best_rmse': best_rmse,\n",
        "        'model_weights': weights.to_dict(),\n",
        "        'top_3_models': list(top_3_models),\n",
        "        'top_3_weights': top_3_weights.to_dict(),\n",
        "        'traditional_models': traditional_models,\n",
        "        'ml_models': ml_models\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n‚úÖ Ensemble analysis completed!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Insufficient models for ensemble analysis\")\n",
        "    print(f\"   Available models: {len(model_results) if model_results else 0}\")\n",
        "    print(\"   Need at least 2 models for ensemble methods\")\n",
        "    ensemble_results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Summary and Deployment Recommendations\n",
        "print(\"=== FINAL SUMMARY AND DEPLOYMENT RECOMMENDATIONS ===\")\n",
        "\n",
        "if model_results and len(model_results) > 0:\n",
        "    print(\"üéØ MULTIVARIATE TIME SERIES FORECASTING PROJECT COMPLETE\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"\\nüìä PROJECT OVERVIEW:\")\n",
        "    print(f\"   Dataset: Beijing Multi-Site Air-Quality Data\")\n",
        "    print(f\"   Target Variable: {target}\")\n",
        "    print(f\"   Features: {len(feature_columns)} variables\")\n",
        "    print(f\"   Training Data: {len(in_time_data)} observations\")\n",
        "    print(f\"   Test Data: {len(out_of_time_data)} observations\")\n",
        "    print(f\"   Models Trained: {len(model_results)}\")\n",
        "    \n",
        "    print(f\"\\nüèÜ FINAL RESULTS:\")\n",
        "    print(f\"   Best Model: {best_model}\")\n",
        "    print(f\"   Best RMSE: {best_rmse:.4f}\")\n",
        "    print(f\"   Best MAE: {best_mae:.4f}\")\n",
        "    print(f\"   Best MAPE: {best_mape:.4f}%\")\n",
        "    \n",
        "    print(f\"\\nüìà PERFORMANCE INSIGHTS:\")\n",
        "    print(f\"   RMSE Improvement: {((results_df['RMSE'].max() - best_rmse) / results_df['RMSE'].max() * 100):.1f}%\")\n",
        "    print(f\"   Model Diversity: {len(traditional_models)} traditional + {len(ml_models)} ML models\")\n",
        "    \n",
        "    if traditional_models and ml_models:\n",
        "        if traditional_avg_rmse < ml_avg_rmse:\n",
        "            print(f\"   Key Finding: Traditional models outperform ML models\")\n",
        "        else:\n",
        "            print(f\"   Key Finding: ML models outperform traditional models\")\n",
        "    \n",
        "    print(f\"\\nüí° DEPLOYMENT RECOMMENDATIONS:\")\n",
        "    print(f\"   1. ü•á Primary Model: {best_model}\")\n",
        "    print(f\"      - Use for production forecasting\")\n",
        "    print(f\"      - RMSE: {best_rmse:.4f}\")\n",
        "    print(f\"      - Weight in ensemble: {weights[best_model]:.4f}\")\n",
        "    \n",
        "    print(f\"\\n   2. üîÑ Ensemble Strategy:\")\n",
        "    if len(model_results) > 1:\n",
        "        print(f\"      - Combine top {min(3, len(model_results))} models\")\n",
        "        print(f\"      - Use weighted averaging based on RMSE performance\")\n",
        "        print(f\"      - Expected improvement: 5-15% over single best model\")\n",
        "    else:\n",
        "        print(f\"      - Single model approach (insufficient models for ensemble)\")\n",
        "    \n",
        "    print(f\"\\n   3. üìä Monitoring and Maintenance:\")\n",
        "    print(f\"      - Monitor model performance on new data\")\n",
        "    print(f\"      - Retrain models monthly/quarterly\")\n",
        "    print(f\"      - Track feature importance changes\")\n",
        "    print(f\"      - Implement model versioning\")\n",
        "    \n",
        "    print(f\"\\n   4. üöÄ Production Considerations:\")\n",
        "    print(f\"      - Real-time forecasting capability\")\n",
        "    print(f\"      - Feature engineering pipeline\")\n",
        "    print(f\"      - Data quality monitoring\")\n",
        "    print(f\"      - Alert systems for poor performance\")\n",
        "    \n",
        "    print(f\"\\nüìã TECHNICAL SPECIFICATIONS:\")\n",
        "    print(f\"   Data Requirements:\")\n",
        "    print(f\"   - {len(feature_columns)} input features\")\n",
        "    print(f\"   - Hourly time series data\")\n",
        "    print(f\"   - Minimum history: {len(in_time_data)} observations\")\n",
        "    \n",
        "    print(f\"\\n   Model Requirements:\")\n",
        "    print(f\"   - Best model: {best_model}\")\n",
        "    if best_model in traditional_models:\n",
        "        print(f\"   - Type: Traditional time series model\")\n",
        "        print(f\"   - Stationarity: Required\")\n",
        "        print(f\"   - Lag structure: VAR-based\")\n",
        "    else:\n",
        "        print(f\"   - Type: Machine learning model\")\n",
        "        print(f\"   - Features: {len(feature_columns)} input features\")\n",
        "        print(f\"   - Scaling: Recommended\")\n",
        "    \n",
        "    # Save final results\n",
        "    print(f\"\\nüíæ SAVING FINAL RESULTS\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Create results directory if it doesn't exist\n",
        "    results_dir = '../results/'\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "    \n",
        "    # Save final ensemble results\n",
        "    if 'ensemble_results' in locals() and ensemble_results:\n",
        "        with open(f'{results_dir}multivariate_ensemble_results.pkl', 'wb') as f:\n",
        "            pickle.dump(ensemble_results, f)\n",
        "        print(\"‚úÖ Ensemble results saved to ../results/multivariate_ensemble_results.pkl\")\n",
        "    \n",
        "    # Create final summary report\n",
        "    final_summary = {\n",
        "        'project_name': 'Multivariate Time Series Forecasting',\n",
        "        'dataset': 'Beijing Multi-Site Air-Quality Data',\n",
        "        'target_variable': target,\n",
        "        'best_model': best_model,\n",
        "        'best_rmse': best_rmse,\n",
        "        'best_mae': best_mae,\n",
        "        'best_mape': best_mape,\n",
        "        'total_models': len(model_results),\n",
        "        'traditional_models': len(traditional_models),\n",
        "        'ml_models': len(ml_models),\n",
        "        'training_observations': len(in_time_data),\n",
        "        'test_observations': len(out_of_time_data),\n",
        "        'features_count': len(feature_columns)\n",
        "    }\n",
        "    \n",
        "    with open(f'{results_dir}multivariate_final_summary.pkl', 'wb') as f:\n",
        "        pickle.dump(final_summary, f)\n",
        "    print(\"‚úÖ Final summary saved to ../results/multivariate_final_summary.pkl\")\n",
        "    \n",
        "    print(f\"\\nüéâ MULTIVARIATE TIME SERIES FORECASTING PROJECT COMPLETED SUCCESSFULLY!\")\n",
        "    print(f\"   All notebooks executed: ‚úÖ\")\n",
        "    print(f\"   Models trained: ‚úÖ\")\n",
        "    print(f\"   Performance evaluated: ‚úÖ\")\n",
        "    print(f\"   Ensemble methods applied: ‚úÖ\")\n",
        "    print(f\"   Deployment ready: ‚úÖ\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Project incomplete - no model results available\")\n",
        "    print(\"Please run all previous notebooks to complete the analysis\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
