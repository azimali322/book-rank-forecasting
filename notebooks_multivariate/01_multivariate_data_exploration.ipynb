{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Multivariate Time Series Data Exploration\n",
    "\n",
    "This notebook focuses on:\n",
    "1. Loading and exploring multivariate time series datasets\n",
    "2. Understanding multiple features per time step\n",
    "3. Feature relationships and correlations\n",
    "4. Time series characteristics and patterns\n",
    "5. Data quality assessment and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries with robust error handling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All imports completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Selection and Loading\n",
    "print(\"=== MULTIVARIATE TIME SERIES DATASET SELECTION ===\")\n",
    "\n",
    "# Available monitoring stations\n",
    "stations = {\n",
    "    'Aotizhongxin': 'PRSA_Data_Aotizhongxin_20130301-20170228.csv',\n",
    "    'Changping': 'PRSA_Data_Changping_20130301-20170228.csv',\n",
    "    'Dingling': 'PRSA_Data_Dingling_20130301-20170228.csv',\n",
    "    'Dongsi': 'PRSA_Data_Dongsi_20130301-20170228.csv',\n",
    "    'Guanyuan': 'PRSA_Data_Guanyuan_20130301-20170228.csv',\n",
    "    'Gucheng': 'PRSA_Data_Gucheng_20130301-20170228.csv',\n",
    "    'Huairou': 'PRSA_Data_Huairou_20130301-20170228.csv',\n",
    "    'Nongzhanguan': 'PRSA_Data_Nongzhanguan_20130301-20170228.csv',\n",
    "    'Shunyi': 'PRSA_Data_Shunyi_20130301-20170228.csv',\n",
    "    'Tiantan': 'PRSA_Data_Tiantan_20130301-20170228.csv',\n",
    "    'Wanliu': 'PRSA_Data_Wanliu_20130301-20170228.csv',\n",
    "    'Wanshouxigong': 'PRSA_Data_Wanshouxigong_20130301-20170228.csv'\n",
    "}\n",
    "\n",
    "# Dataset configuration\n",
    "dataset_config = {\n",
    "    'name': 'Beijing Multi-Site Air-Quality Data',\n",
    "    'features': ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM'],\n",
    "    'target': 'PM2.5',\n",
    "    'time_col': 'datetime'\n",
    "}\n",
    "\n",
    "print(\"Available monitoring stations:\")\n",
    "for i, (station_name, filename) in enumerate(stations.items(), 1):\n",
    "    print(f\"{i:2d}. {station_name}\")\n",
    "print()\n",
    "\n",
    "# Select station (default to Aotizhongxin)\n",
    "selected_station = 'Aotizhongxin'  # Change this to select a different station\n",
    "selected_file = stations[selected_station]\n",
    "\n",
    "print(f\"Selected station: {selected_station}\")\n",
    "print(f\"File: {selected_file}\")\n",
    "print(f\"Target variable: {dataset_config['target']}\")\n",
    "print(f\"Number of features: {len(dataset_config['features'])}\")\n",
    "\n",
    "# Create dataset info for compatibility\n",
    "dataset_info = {\n",
    "    'name': f\"{dataset_config['name']} - {selected_station}\",\n",
    "    'file': f'../data_multivariate/{selected_file}',\n",
    "    'features': dataset_config['features'],\n",
    "    'target': dataset_config['target'],\n",
    "    'time_col': dataset_config['time_col'],\n",
    "    'station': selected_station\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the selected dataset\n",
    "print(f\"=== LOADING {dataset_info['name'].upper()} ===\")\n",
    "\n",
    "try:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(dataset_info['file'])\n",
    "    print(f\"✅ Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display basic info\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ File not found: {dataset_info['file']}\")\n",
    "    print(\"Please run the download script first:\")\n",
    "    print(\"python ../data_multivariate/download_multivariate_data.py\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and Time Series Setup\n",
    "print(\"=== DATA PREPROCESSING AND TIME SERIES SETUP ===\")\n",
    "\n",
    "if 'df' in locals() and not df.empty:\n",
    "    # Handle time column\n",
    "    time_col = dataset_info['time_col']\n",
    "    \n",
    "    # Convert to datetime (Beijing air quality data has separate year, month, day, hour columns)\n",
    "    if 'year' in df.columns and 'month' in df.columns and 'day' in df.columns and 'hour' in df.columns:\n",
    "        df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])\n",
    "        df = df.set_index('datetime')\n",
    "        print(f\"✅ Time columns converted to datetime and set as index\")\n",
    "        print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "        print(f\"Time span: {df.index.max() - df.index.min()}\")\n",
    "    else:\n",
    "        print(f\"❌ Time columns not found in dataset\")\n",
    "        print(f\"Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percent = (missing_values / len(df)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_values,\n",
    "        'Missing Percentage': missing_percent\n",
    "    })\n",
    "    \n",
    "    print(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    if missing_df['Missing Count'].sum() == 0:\n",
    "        print(\"✅ No missing values found!\")\n",
    "    \n",
    "    # Select features and target\n",
    "    features = dataset_info['features']\n",
    "    target = dataset_info['target']\n",
    "    \n",
    "    # Check if features exist in dataset\n",
    "    available_features = [f for f in features if f in df.columns]\n",
    "    missing_features = [f for f in features if f not in df.columns]\n",
    "    \n",
    "    print(f\"\\nFeature Availability:\")\n",
    "    print(f\"Available features: {len(available_features)}/{len(features)}\")\n",
    "    print(f\"Available: {available_features}\")\n",
    "    if missing_features:\n",
    "        print(f\"Missing: {missing_features}\")\n",
    "    \n",
    "    # Check target variable\n",
    "    if target in df.columns:\n",
    "        print(f\"✅ Target variable '{target}' found\")\n",
    "        print(f\"Target statistics: {df[target].describe()}\")\n",
    "    else:\n",
    "        print(f\"❌ Target variable '{target}' not found\")\n",
    "        print(f\"Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Create feature matrix\n",
    "    if available_features and target in df.columns:\n",
    "        feature_cols = available_features + [target]\n",
    "        df_features = df[feature_cols].copy()\n",
    "        \n",
    "        print(f\"\\n✅ Feature matrix created with {len(feature_cols)} columns\")\n",
    "        print(f\"Feature matrix shape: {df_features.shape}\")\n",
    "    else:\n",
    "        print(\"❌ Cannot create feature matrix - missing features or target\")\n",
    "        df_features = df.copy()\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No dataset loaded. Please run the previous cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
